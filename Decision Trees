library(rpart)
library(ggplot2)
library(rpart.plot)


#Find total number of observations
n<-NROW(mpg)

#Create a vector allocating each observation to train #or test 
train_or_test<-ifelse(runif(n)<0.7,'Train','Test') 

#Add to mpg data frame 
mpg_exp<-add_column(mpg,Sample=train_or_test) 

#Isolate Training Data 
mpg_train<-filter(mpg_exp,Sample=='Train')

#Isolate Test Data
mpg_test<-filter(mpg_exp,Sample=='Test')

#Default Settings 画小树
rpart_small<-rpart(drv~displ+hwy,data = mpg_train). ## drv as the target variable, displ+hwy as the predictor

#Bigger tree
#Allow for partitions with as few as two #training observations
#Accept any split that improves fit 
rpart_big<-rpart(drv~displ+hwy,data = mpg_train,
                 control = rpart.control(minbucket=2, cp=0))
#Make predictions
pred_small<-predict(rpart_small,mpg_test,type='class')
pred_big<-predict(rpart_big,mpg_test,type='class')

##To compute test misclassification
mean(pred_small!=mpg_test$drv)
mean(pred_big!=mpg_test$drv)

table(pred_small,mpg_test$drv)

##In the bigger tree perform better out-of-sample (this is rare).

## Probabilities

pred_small<-predict(rpart_small,mpg_test) 
pred_small

##Plotting the trees 画树

rpart.plot(rpart_small,extra = 0,type = 0)
rpart.plot(rpart_big,extra = 0,type = 0)

##rpart.plot actually provides more information. 

rpart.plot(rpart_small)


################### 练习 ############

#Use MASS package

library(rpart)

#Later tidyverse also used

library(tidyverse)

#Load in data
ExistingWines<-readRDS('ExistingWines.rds')

#Load in New Wine Data 

NewWines<-readRDS('NewWines.rds')
tree<-rpart(BestMarket~.,data = ExistingWines)

##画树
library(rpart.plot)
rpart.plot(tree)

yhat<-predict(tree,newdata = NewWines,type='class')  —————预测 NewWines 

#The first ten predictions can be checked using the head function

head(yhat,10)

## Requiring at least 15 training observations to be within each partition 

tree15<-rpart(BestMarket~., data = ExistingWines,
    control = rpart.control(minbucket = 15))
    rpart.plot(tree15)

#Get predictions

yhat15<-predict(tree15,NewWines,type='class')
  
#The first ten predictions can be checked using the head function

head(yhat15,10)

## 随机自主创建数据
#This is the same problem as last week. However since the lda and qda functions take in the data differently to the knn function
#Create an indicator that determines whether it is training or test sample.

ind<-ifelse(runif(125)<0.7,"Training Sample","Test Sample")

#A data set augmented with sample information

Data_with_Sample<-add_column(ExistingWines,Sample=ind)

#Get Training data

train_data<-Data_with_Sample%>%
  filter(Sample=="Training Sample")%>%
  select(-Sample)                         #Can remove Sample variable

#Get Test data

test_data<-Data_with_Sample%>%
  filter(Sample=="Test Sample")%>%
  select(-Sample)                        #Can remove Sample variable


## 正常的树和minimum 15 observations 比较好坏
## Tree without minimum 15 observations per bin

tree<-rpart(BestMarket~.,data = train_data)

yhat<-predict(tree,test_data,type='class')

# Tree with minimum 15 observations per bin

tree15<-rpart(BestMarket~.,data = train_data,
    control = rpart.control(minbucket = 15))
    
yhat15<-predict(tree15,test_data,type='class') ————测试

#Compare misclassification rate

#Default tree
mean(yhat!=test_data$BestMarket) ——————检验

#Default tree
mean(yhat15!=test_data$BestMarket)   ——————检验

#For this particular example the more complicated tree performs better. Both perform worse
#than LDA and QDA and k-NN with k=5. The more complicated tree performs equally to kNN with k=1.





