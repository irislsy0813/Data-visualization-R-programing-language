library(MASS)
library(ggplot2)
library(tidyverse)
library(viridis)

n<-nrow(mpg)
train_or_test<-ifelse(runif(n)<0.7,'Train','Test')  ##把所有数据分成train和test，runif min = 0, max = 1随机数。
table(train_or_test)  ##table可以查看不同变量总数

mpg_exp<-add_column(mpg,Sample=train_or_test)
view(mpg_exp)

mpg_train<-filter(mpg_exp,Sample=='Train')
mpg_test<-filter(mpg_exp,Sample=='Test')

summary(mpg_train)
summary(mpg_test)  ##两者的mean很接近即为好

##lda a

lda_out<-lda(drv~displ+hwy,data = mpg_train)##找规律

ldapred<-predict(lda_out,mpg_test)##对实验数据mpg_test做预测

##Missclasification Rate

mean(ldapred$class!=mpg_test$drv) ##错误概率

table(ldapred$class,mpg_test$drv)

### qda

qda_out<-qda(drv~displ+hwy,data = mpg_train) ##找规律

qdapred<-predict(qda_out,mpg_test) ##对实验数据mpg_test做预测

##Missclasification Rate

mean(qdapred$class!=mpg_test$drv)

table(qdapred$class,mpg_test$drv)

table(qdapred$class,ldapred$class) ##row, col


### Evaluate whether the assumptions of multivariate normality and homogenous variances and covariances hold.

mpg_train%>%
  group_by(drv)%>%
  summarise(VarDispl=var(displ),
            VarHwy=var(hwy),
            covDisplHwy=cov(displ,hwy))->varcov 
## `summarise()` ungrouping output (override with `.groups` argument)

mpg_train%>%
  ggplot(aes(x=displ,y=hwy,col=drv))+
  geom_density2d()+
  scale_color_colorblind()

####
Double Cross Validation（2-fold Cross Validation，记为2-CV）
做法是将数据集分成两个相等大小的子集，进行两回合的分类器训练。在第一回合中，一个子集作为training set，另一个便作为testing set；
在第二回合中，则将training set与testing set对换后，再次训练分类器，而其中我们比较关心的是两次testing sets的辨识率。
不过在实务上2-CV并不常用，主要原因是training set样本数太少，通常不足以代表母体样本的分布，导致testing阶段辨识率容易出现明显落差。
此外，2-CV中分子集的变异度大，往往无法达到“实 验过程必须可以被复制”的要求。
